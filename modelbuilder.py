import torch
import torch.nn as nn
import torch.nn.functional as F
import random

# Custom loss to ignore the <sos> tag on the target. This comes from the way the number of sequences is
# calculated in the Seq2Seq model.
def seq2seq_loss(input, target):
    sl_in,bs_in,nc = input.size()
    return F.cross_entropy(input.view(-1,nc), target[1:].view(-1), ignore_index=1)

# Replace the data in the embeddings generated by PyTorch with pretrained ones.
def create_emb(pretrained_vecs, itos, emb_sz):
    # Creates a randomly initialized embedding matrix of vocab size x embedding dimension
    emb = nn.Embedding(len(itos), emb_sz, padding_idx=1)
    wgts = emb.weight.data
    miss = []
    for i,w in enumerate(itos):
        # For each word in vocab, if it exists in the pretrained vocab (wiki/glove/word2vec etc), then replace
        # its index in the embedding weight matrix.
        if w in pretrained_vecs:
            wgts[i] = torch.from_numpy(pretrained_vecs[w])
    return emb

# Encoder Class
class Encoder(nn.Module):
    def __init__(self, pretrained_vecs, itos, emb_dim, hid_dim, n_layers, lstm_dropout, emb_dropout, bidir):
        super().__init__()
        
        self.emb_dim = emb_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        self.bidir = bidir
        
        self.embedding = create_emb(pretrained_vecs, itos, emb_dim)
        
        self.emb_dropout = nn.Dropout(emb_dropout)
        
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=lstm_dropout, bidirectional=bidir)
                
    def forward(self, src):
        embedded = self.emb_dropout(self.embedding(src))
        outputs, (hidden, cell) = self.rnn(embedded)
        # Need to do some resizing here from bidirectional encoder
        # See https://github.com/pytorch/pytorch/issues/3587 for details on stacked return from LSTM
        # h, c are of size (n_direcionts*n_layers, bs, hidden units), and need to reshape it to 
        # (n_layers, bs, hidden units) for the decoder. The results of the fwd and bwd direction are stacked in
        # dim 1, so we can just sum like so:
        if self.bidir:
            hidden = hidden[:self.n_layers,:,:] + hidden[self.n_layers:,:,:]
            cell = cell[:self.n_layers,:,:] + cell[self.n_layers:,:,:]
            outputs = outputs[:,:,:self.hid_dim] + outputs[:,:,self.hid_dim:]
        
        return outputs, hidden, cell

# Decoder Class
class Decoder(nn.Module):
    def __init__(self, pretrained_vecs, itos, emb_dim, hid_dim, n_layers, lstm_dropout, emb_dropout, fc_dropout):
        super().__init__()

        self.emb_dim = emb_dim
        self.hid_dim = hid_dim
        self.output_dim = len(itos)
        self.n_layers = n_layers
        
        self.embedding = create_emb(pretrained_vecs, itos, emb_dim)
    
        self.emb_dropout = nn.Dropout(emb_dropout)
        
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=lstm_dropout)
        
        self.out = nn.Linear(hid_dim, self.output_dim)
        
        self.fc_dropout = nn.Dropout(fc_dropout)
        
    def forward(self, input, hidden, cell):
        
        input = input.unsqueeze(0)
        
        embedded = self.emb_dropout(self.embedding(input))
                
        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        
        output = output.squeeze(0)
        output = self.fc_dropout(output)
        prediction = self.out(output)
        
        return prediction, hidden, cell

# Seq2seq model class
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        
        batch_size = trg.shape[1]
        max_len = trg.shape[0]

        res = []
        
        #last hidden state of the encoder is used as the initial hidden state of the decoder
        outputs, hidden, cell = self.encoder(src)
        
        #first input to the decoder is the <sos> tokens. This is the first row of the target matrix,
        # <sos> was put in there by torchtext.
        input = trg[0,:]

        for t in range(1, max_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            res.append(output) #outputs[t] = output
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.max(1)[1]
            input = (trg[t] if teacher_force else top1)
        
        return torch.stack(res) #return outputs - Takes a list of tensors and stacks them into one tensor