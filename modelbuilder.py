import torch
import torch.nn as nn
import torch.nn.functional as F
import random

# Custom loss to ignore the <sos> tag on the target. This comes from the way the number of sequences is
# calculated in the Seq2Seq model.
def seq2seq_loss(input, target):
    sl_in,bs_in,nc = input.size()
    return F.cross_entropy(input.view(-1,nc), target[1:].view(-1), ignore_index=1)

# Replace the data in the embeddings generated by PyTorch with pretrained ones.
def create_emb(pretrained_vecs, itos, emb_sz):
    # Creates a randomly initialized embedding matrix of vocab size x embedding dimension
    emb = nn.Embedding(len(itos), emb_sz, padding_idx=1)
    wgts = emb.weight.data
    for i,w in enumerate(itos):
        # For each word in vocab, if it exists in the pretrained vocab (wiki/glove/word2vec etc), then replace
        # its index in the embedding weight matrix.
        if w in pretrained_vecs:
            wgts[i] = torch.from_numpy(pretrained_vecs[w])
    return emb

# Encoder Class
class Encoder(nn.Module):
    def __init__(self, pretrained_vecs, itos, opt):
        super().__init__()
        
        self.emb_dim = opt.emb_dim
        self.hid_dim = opt.num_hid
        self.n_layers = opt.num_layers
        self.bidir = opt.bidir
        
        self.embedding = create_emb(pretrained_vecs, itos, self.emb_dim)
        self.embedding.weight.requires_grad = False
                
        self.rnn = nn.LSTM(self.emb_dim, self.hid_dim, self.n_layers, dropout=opt.cell_drop, bidirectional=opt.bidir)
                
    def forward(self, src):
        embedded = self.embedding(src)
        outputs, (hidden, cell) = self.rnn(embedded)
        # Need to do some resizing here from bidirectional encoder
        # See https://github.com/pytorch/pytorch/issues/3587 for details on stacked return from LSTM
        # h, c are of size (n_direcionts*n_layers, bs, hidden units), and need to reshape it to 
        # (n_layers, bs, hidden units) for the decoder. The results of the fwd and bwd direction are stacked in
        # dim 1, so we can just sum like so:
        if self.bidir:
            hidden = hidden[:self.n_layers,:,:] + hidden[self.n_layers:,:,:]
            cell = cell[:self.n_layers,:,:] + cell[self.n_layers:,:,:]
            outputs = outputs[:,:,:self.hid_dim] + outputs[:,:,self.hid_dim:]
        
        return outputs, hidden, cell

# Decoder Class
class Decoder(nn.Module):
    def __init__(self, pretrained_vecs, itos, opt): #hid_dim, n_layers, lstm_dropout):
        super().__init__()

        self.emb_dim = opt.emb_dim
        self.hid_dim = opt.num_hid
        self.output_dim = len(itos)
        self.n_layers = opt.num_layers
        
        self.embedding = create_emb(pretrained_vecs, itos, self.emb_dim)
        self.embedding.weight.requires_grad = False
        
        self.rnn = nn.LSTM(self.emb_dim, self.hid_dim, self.n_layers, dropout=opt.cell_drop)
        
    def forward(self, input, hidden, cell):
        
        input = input.unsqueeze(0)
        
        embedded = self.embedding(input)
                
        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        
        output = output.squeeze(0)
        
        return output, hidden, cell

# Calculates all the context stuff. Returns a tensor that is the same size as what came out of the decoder
# Meaning it can be passed in directly to the generator.
class Attention(nn.Module):
    def __init__(self, dim):
        super().__init__()
        # Linear input layer with no Bias
        self.linear_in = nn.Linear(dim, dim, bias=False)
        # Linear output layer
        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
        
    def score(self, h_t, h_s):
        src_batch, src_len, src_dim = h_s.size()
        tgt_batch, tgt_len, tgt_dim = h_t.size()
        
        h_t_ = h_t.view(tgt_batch * tgt_len, tgt_dim)
        h_t_ = self.linear_in(h_t_)
        h_t = h_t_.view(tgt_batch, tgt_len, tgt_dim)
        h_s_ = h_s.transpose(1, 2)
        # (batch, t_len, d) x (batch, d, s_len) --> (batch, t_len, s_len)
        return torch.bmm(h_t, h_s_)
        
    
    def forward(self, dec_out, enc_out):
        # dec_out - [batch_size x tgt_len x hidden_dim]
        # enc_out - [batch_size x src_len x hidden_dim]
        batch, source_l, dim = enc_out.size()
        _, target_l, _ = dec_out.size()
        
        align = self.score(dec_out, enc_out)
        align_vectors = F.softmax(align.view(batch*target_l, source_l), -1)
        align_vectors = align_vectors.view(batch, target_l, source_l)
        
        # each context vector c_t is the weighted average
        # over all the source hidden states
        c = torch.bmm(align_vectors, enc_out)
        
        # concatenate
        concat_c = torch.cat([c, dec_out], 2).view(batch*target_l, dim*2)
        attn_h = self.linear_out(concat_c).view(batch, target_l, dim)
        attn_h = torch.tanh(attn_h)
        
        attn_h = attn_h.transpose(0, 1).contiguous()
        align_vectors = align_vectors.transpose(0, 1).contiguous()
        
        return attn_h, align_vectors

# Seq2seq model class
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, num_hidden, tgt_vocab_size, final_dropout):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        # Add the generator object later.
        self.generator = nn.Sequential(
                            nn.Linear(num_hidden, tgt_vocab_size),
                            nn.LogSoftmax(dim=-1)
                            )
        # Attention
        self.attention = Attention(num_hidden)
        
        # Final dropout layer
        self.final_dropout = nn.Dropout(final_dropout)
        
    def forward(self, src, tgt, teacher_forcing_ratio=0.5):
        
        batch_size = tgt.shape[1]
        max_len = tgt.shape[0]

        res = []
        
        #last hidden state of the encoder is used as the initial hidden state of the decoder
        enc_outputs, hidden, cell = self.encoder(src)

        #first input to the decoder is the <sos> tokens. This is the first row of the target matrix,
        # <sos> was put in there by torchtext.
        input = tgt[0,:]

        for t in range(1, max_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            res.append(output) #outputs[t] = output
            # Teacher forcing
            if random.random() < teacher_forcing_ratio:  
                input = tgt[t]
            else:
                input = self.generator(output)
                input = input.max(1)[1]
        
        # Have a 
        dec_outputs = torch.stack(res)
#         print(f'Output from Dcoder shape: {r.shape}') #(output seq length, batch size, hidden dim)
        outputs, attns = self.attention(dec_outputs.transpose(0,1).contiguous(), enc_outputs.transpose(0,1))
        outputs = self.final_dropout(outputs)
        return outputs #, attns #Don't need to return the attentions yet